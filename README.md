# ğŸ¤– Ejecucion de Modelos IA

Â¡Bienvenido! Este repositorio contiene tres proyectos desarrollados con modelos de Hugging Face:

1. ğŸ§  **AplicaciÃ³n de Preguntas y Respuestas**.
2. ğŸ”„ **IteraciÃ³n entre mÃºltiples modelos LLM**.
3. ğŸ–¼ï¸ **DetecciÃ³n de objetos usando un modelo local**.

---

## ğŸš€ InstalaciÃ³n y Uso

### Requisitos
- Python 3.9+
- pip
- LibrerÃ­as detalladas en `requirements.txt`.

### InstalaciÃ³n

```bash
git clone https://github.com/usuario/huggingface-projects.git

cd huggingface-projects

python -m venv env

.\env\Scripts\activate

pip install -r requirements.txt
```

## ğŸ“š DescripciÃ³n de los Proyectos y su ejecuciÃ³n

### ğŸ§  Proyecto 1: AplicaciÃ³n de Preguntas y Respuestas
**Objetivo:** Crear una aplicaciÃ³n que permita al usuario realizar preguntas y recibir respuestas utilizando modelos de Hugging Face.


#### Ejecutar
```bash
streamlit run proyecto1/app.py
```
![alt text](image.png)


### ğŸ”„ Proyecto 2: Iterar entre MÃºltiples Modelos
**Objetivo:** Implementar mÃºltiples modelos y permitir la iteraciÃ³n entre ellos mediante una clase. 
En este caso un modelo de traducciÃ³n de ingles a frances y un modelo de resumen de texto.

#### Ejecutar
```bash
streamlit run proyecto2/multi_model.py
```
![alt text](image-1.png)


### ğŸ–¼ï¸ Proyecto 3: Modelo Local
**Objetivo:** Ejecutar un modelo localmente, como identificaciÃ³n de objetos en imÃ¡genes.
#### Ejecutar
```bash
python proyecto3/object_detection.py
```


## SecciÃ³n Especial: Transformers y Modelos Locales

### ğŸ¤” Â¿QuÃ© son los Transformers?

Los transformers son una arquitectura de red neuronal que ha revolucionado el procesamiento del lenguaje natural y otras Ã¡reas. Se basan en mecanismos de atenciÃ³n que permiten a los modelos enfocarse en diferentes partes de los datos de entrada.


## ValidaciÃ³n de Recursos

**Memoria requerida:** Depende del modelo. Por ejemplo:

`bert-base-uncased: ~440MB.`
`gpt-neo: ~1.2GB.`

**CÃ¡lculo:** (nÃºmero de parÃ¡metros * tamaÃ±o de flotante). Ejemplo:
Un modelo con 110M de parÃ¡metros y flotantes de 32 bits: `110e6 * 4 bytes = 440MB.`

---

### ğŸ› ï¸ ConfiguraciÃ³n Local

Para usar modelos localmente:

1. Descarga el modelo desde Hugging Face:

    ```bash
    transformers-cli download <modelo>
    ```

2. Carga el modelo en tu script:

    ```python
    from transformers import AutoModel
    model = AutoModel.from_pretrained('ruta_a_modelo_local')
    ```